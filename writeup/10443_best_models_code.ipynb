{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Code For Best Models\n",
    "> This notebook contains the code for the best 2 models developed\n",
    "\n",
    "**NOTES:**  \n",
    "\n",
    "1. First step will be to run `sh startup.sh` from the terminal to install all necessary dependencies  \n",
    "2. The deep learning model requires a GPU to run\n",
    "3. Several code lines commented out are not unwanted. Instead, uncommenting them could create different models\n",
    "\n",
    "**A complete documentation of all models included is stored in [the github repository](https://github.com/lse-my474/classification-challenge-ry05/tree/main/code)**\n",
    "\n",
    "The outputs of the hyperparameter tuning and also the ML model performances are documented in [here](https://github.com/lse-my474/classification-challenge-ry05/tree/main/code/models)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Best Machine Learning Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filename: pipelines.py\n",
    "\n",
    "Not runnable individually. Preprocessing techniques used.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "Preprocessing script\n",
    "--------------------\n",
    "\n",
    "This script contains elements that help in preprocessing the text\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import texthero as hero\n",
    "from texthero import stopwords, preprocessing \n",
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "\n",
    "DEF_STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', \"don't\", 'should', \"should've\", 'now', \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"wouldn't\"]\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    \"\"\"\n",
    "    Stems tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "    def __call__(self, comment):\n",
    "        return [self.stemmer.stem(token) for token in word_tokenize(comment)]\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    \"\"\"Lemmatizes tokens\n",
    "    Source: https://stackoverflow.com/questions/47423854/sklearn-adding-lemmatizer-to-countvectorizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, comment):\n",
    "        return [self.lemmatizer.lemmatize(token) for token in word_tokenize(comment)]\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "    Removes punctuations from text\n",
    "    :param text: text (str)\n",
    "    :return: text with no punctuations\n",
    "    \"\"\"\n",
    "\n",
    "    return  re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "\n",
    "def bad_spaces(text):\n",
    "    \"\"\"\n",
    "    Remove bad spaces\n",
    "    :param text: text (str)\n",
    "    :return: text with no bad spacing\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = text.split()\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "def make_lower(text):\n",
    "    \"\"\"\n",
    "    Convert to lowercase\n",
    "    :param text: text (str)\n",
    "    :return: text in lowercase\n",
    "    \"\"\"\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def translate_text(text):\n",
    "    \"\"\"\n",
    "    Translate the text by performing \"special operations\"\n",
    "    :param text: text (str)\n",
    "    :return: translated text\n",
    "    \"\"\"\n",
    "\n",
    "    # corpus stopwords obtained from data exploration\n",
    "    corpus_stopwords = ['fuck', 'fag', 'faggot', 'fggt', 'nigga', 'nigger', 'aids', 'article', 'page', 'wiki', 'wp', 'block', 'NOES', 'ANONYMOUS', 'UTC', 'NOT', 'OH', 'IP', 'POV', 'LIVE', 'WP', 'REDIRECT', 'BTW', 'AIDS', 'HUGE', 'BLEACHANHERO', 'PHILIPPINESLONG']\n",
    "    cs_lower = [s.lower() for s in corpus_stopwords]\n",
    "    cs_upper = [s.upper() for s in corpus_stopwords]\n",
    "\n",
    "    you_tokens = ['you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "    stop_tokens = DEF_STOPWORDS\n",
    "    \n",
    "    # remove punctuations\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "\n",
    "    # remove corpus stopwords\n",
    "    # removing these won't affect as the presence of necessary words have been computed in data exploration\n",
    "    # and the dataset is stored\n",
    "    text_tokens = text.split()\n",
    "    text_tokens = [tok for tok in text_tokens if ((tok not in cs_lower) and (tok not in cs_upper))]\n",
    "    translated_tokens = []\n",
    "\n",
    "    # add labels to select groups of words\n",
    "    for token in text_tokens:\n",
    "        if token in you_tokens:\n",
    "            translated_tokens.append(\"YOUWORD\")\n",
    "        elif token in stop_tokens:\n",
    "            translated_tokens.append(\"STOPWORD\")\n",
    "        else:\n",
    "            translated_tokens.append(token)\n",
    "\n",
    "    translated_text = \" \".join(translated_tokens)\n",
    "\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filename: pipelines.py\n",
    "\n",
    "Not runnable individually. Stores the machine learning pipelines.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "Text Pipelines\n",
    "--------------\n",
    "\n",
    "The use of a `pipeline` is to chain several data operations together and\n",
    "automate some or most parts of the ML process\n",
    "Advantages of using a pipeline\n",
    "1. Convenience\n",
    "2. Prevents leakage\n",
    "3. Easier hyperparameter tuning\n",
    "Source: https://scikit-learn.org/stable/modules/compose.html\n",
    "\n",
    "All pipelines in this file set after performing hyperparameter optimization\n",
    "The code for hyperparameter optimization for the best model\n",
    "is presented in `ml_best_hyp_opt.py`\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from fastai.text.all import *\n",
    "\n",
    "from preprocess import StemTokenizer\n",
    "from feature_selection import UnivariateFeatureSelection\n",
    "\n",
    "# fastai's tokenizer function\n",
    "spacy = WordTokenizer()\n",
    "tokenizer = Tokenizer(spacy)\n",
    "\n",
    "# pipeline 1\n",
    "pipeline_1 = Pipeline([\n",
    "    ('countvec', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# pipeline 2\n",
    "pipeline_2 = Pipeline([\n",
    "    ('countvec', CountVectorizer(\n",
    "        stop_words = 'english'\n",
    "    )),\n",
    "    ('clf', ComplementNB())\n",
    "])\n",
    "\n",
    "# pipeline 3\n",
    "pipeline_3 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        tokenizer = StemTokenizer(),\n",
    "        # tokenizer = tokenizer,\n",
    "        # token_pattern = None,\n",
    "        # lowercase = False,\n",
    "        max_features=10000\n",
    "    )),\n",
    "    ('ufs', UnivariateFeatureSelection(\n",
    "        n_features = 0.05,     # Top 5% of the features built\n",
    "        scoring = 'chi2'     \n",
    "    )),\n",
    "    ('clf', ComplementNB(alpha=0.01))     # classifier\n",
    "])\n",
    "\n",
    "# pipeline 4\n",
    "numeric_transformer = Pipeline([\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# transformer for text feature\n",
    "# use string as vectorizer converts a single vector into multiple vectors\n",
    "text_transformer = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        # tokenizer = StemTokenizer(),\n",
    "        # token_pattern = None,\n",
    "        tokenizer = tokenizer,\n",
    "        # lowercase = False,\n",
    "        max_features=10000\n",
    "    )),\n",
    "    ('ufs', UnivariateFeatureSelection(\n",
    "        n_features = 0.05,     # Top 5% of the features built\n",
    "        scoring = 'chi2'     \n",
    "    ))\n",
    "])\n",
    "\n",
    "# hardcoded\n",
    "numeric = ['afinn', 'you_count', 'caps_word_count', 'digits_count', 'dale_chall']\n",
    "binary = ['source_cnt', 'f*g_cnt', 'n***_cnt', 'fu**_cnt', 'article_cnt', 'REDIRECT_count'] # these are not preprocessed\n",
    "text = 'text'\n",
    "\n",
    "# preprocessor for heterogenous data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', numeric_transformer, numeric),\n",
    "        ('text', text_transformer, text)\n",
    "])\n",
    "\n",
    "# integrate it all into the pipeline\n",
    "pipeline_4 = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', ComplementNB(alpha=0.5)) # ComplementNB was used\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline dictionary\n",
    "pipe_dict = {\n",
    "    '1': pipeline_1,\n",
    "    '2': pipeline_2,\n",
    "    '3': pipeline_3,\n",
    "    '4': pipeline_4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filename: train2.py\n",
    "\n",
    "To run from terminal run this command: python train2.py --pipeline_number 4 --data with_numeric\n",
    "Output style: 5-fold CV performance as tables\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "Training script\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import config\n",
    "import pipelines as pipe\n",
    "from preprocess import translate_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # take in arguments from the terminal\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--pipeline_number\",\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data\",\n",
    "        type=str\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    target = config.TARGET\n",
    "    text_feat = config.TEXT\n",
    "\n",
    "    # get the data\n",
    "    if args.data == 'raw':\n",
    "        train = pd.read_csv(config.TRAIN_RAW)\n",
    "        train = train.fillna('missing')\n",
    "        print('Preview of data')\n",
    "        print(train.head())\n",
    "        print()\n",
    "        # split into independent and dependent variables\n",
    "        train_x = train[text_feat]\n",
    "        train_y = train[target].values\n",
    "    elif args.data == 'with_numeric':\n",
    "        train = pd.read_csv(config.TRAIN_NUMERIC)\n",
    "        train = train.fillna('missing')\n",
    "        print('Preview of data')\n",
    "        print(train.head())\n",
    "        print()\n",
    "        # split into independent and dependent variables\n",
    "        train_x = train.drop(['id', 'attack'], axis=1)\n",
    "        train_y = train[target].values\n",
    "    elif args.data == 'with_numeric_translated_text':\n",
    "        train = pd.read_csv(config.TRAIN_NUMERIC)\n",
    "        train = train.fillna('missing')\n",
    "        print('Preview of data')\n",
    "        print(train.head())\n",
    "        print()\n",
    "        # split into independent and dependent variables\n",
    "        train[text_feat] = train[text_feat].apply(translate_text)\n",
    "        train_x = train.drop(['id', 'attack'], axis=1)\n",
    "        train_y = train[target].values    \n",
    "\n",
    "    # instantiate the estimator\n",
    "    if int(args.pipeline_number) < 4:\n",
    "        if args.data != 'raw':\n",
    "            print('Thou shall not use this combination of pipeline and data!')\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            estimator = pipe.pipe_dict[args.pipeline_number]\n",
    "    elif int(args.pipeline_number) == 4:\n",
    "        print(\"Going ahead with this hoping you have rechecked `pipelines.py` w.r.t features considered!\")\n",
    "        if args.data == 'raw':\n",
    "            print('Thou shall not use this combination of pipeline and data!')\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            estimator = pipe.pipe_dict[args.pipeline_number]\n",
    "    else:\n",
    "        print('The pipeline or data or both have not yet been created!')\n",
    "        sys.exit(0)\n",
    "\n",
    "    print(f'Training {args.data} data with pipeline {args.pipeline_number} using 5-fold stratified cross validation...')\n",
    "    print()\n",
    "\n",
    "    # setup 5-fold stratified cross validation\n",
    "    # stratified due to the presence of class imbalance\n",
    "    cv_scores = cross_validate(estimator, train_x, train_y, cv=5, scoring=['f1', 'accuracy'], error_score='raise')\n",
    "    performance_df = pd.DataFrame(\n",
    "        dict(\n",
    "            fit_time = cv_scores['fit_time'],\n",
    "            score_time = cv_scores['score_time'],\n",
    "            validation_acc = cv_scores['test_accuracy'],\n",
    "            validation_f1 = cv_scores['test_f1']\n",
    "        )\n",
    "    )\n",
    "    print(f'Performance Table')\n",
    "    print('(All times in seconds)')\n",
    "    print()\n",
    "    print(performance_df)\n",
    "    print()\n",
    "    print(f'The mean CV F1 score is {performance_df.validation_f1.mean()}')\n",
    "    print(f'The mean CV accuracy for 5-fold CV is {performance_df.validation_acc.mean()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filename: inference2.py\n",
    "\n",
    "To run from terminal run this command: python inference2.py --pipeline_number 4 --data with_numeric\n",
    "Output style: submission.csv file generated in ../data with predited labels for test data\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "Inference script\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import config\n",
    "import pipelines as pipe\n",
    "from preprocess import translate_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--pipeline_number\",\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data\",\n",
    "        type=str\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    target = config.TARGET\n",
    "    text_feat = config.TEXT\n",
    "\n",
    "    # get the data\n",
    "    if args.data == 'raw':\n",
    "        train = pd.read_csv(config.TRAIN_RAW)\n",
    "        test = pd.read_csv(config.TEST_RAW)\n",
    "        train = train.fillna('missing')\n",
    "        test = test.fillna('missing')\n",
    "        # split into independent and dependent variables\n",
    "        train_x = train[text_feat]\n",
    "        train_y = train[target].values\n",
    "        test_x = test[text_feat]\n",
    "    elif args.data == 'with_numeric':\n",
    "        train = pd.read_csv(config.TRAIN_NUMERIC)\n",
    "        test = pd.read_csv(config.TEST_NUMERIC)\n",
    "        train = train.fillna('missing')\n",
    "        test = test.fillna('missing')\n",
    "        # split into independent and dependent variables\n",
    "        train_x = train.drop(['id'], axis=1)\n",
    "        train_y = train[target].values\n",
    "        test_x = test[text_feat]\n",
    "    elif args.data == 'with_numeric_translated_text':\n",
    "        train = pd.read_csv(config.TRAIN_NUMERIC)\n",
    "        test = pd.read_csv(config.TEST_NUMERIC)\n",
    "        train = train.fillna('missing')\n",
    "        test = test.fillna('missing')\n",
    "        # split into independent and dependent variables\n",
    "        train[text_feat] = train[text_feat].apply(translate_text)\n",
    "        test[text_feat] = test[text_feat].apply(translate_text)\n",
    "        train_x = train.drop(['id'], axis=1)\n",
    "        train_y = train[target].values \n",
    "        test_x = train.drop(['id'], axis=1)  \n",
    "\n",
    "    # instantiate the estimator\n",
    "    if int(args.pipeline_number) < 4:\n",
    "        if args.data != 'raw':\n",
    "            print('Thou shall not use this combination of pipeline and data!')\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            estimator = pipe.pipe_dict[args.pipeline_number]\n",
    "    elif int(args.pipeline_number) == 4:\n",
    "        print(\"Going ahead with this hoping you have rechecked `pipelines.py` w.r.t features considered!\")\n",
    "        if args.data == 'raw':\n",
    "            print('Thou shall not use this combination of pipeline and data!')\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            estimator = pipe.pipe_dict[args.pipeline_number]\n",
    "    else:\n",
    "        print('The pipeline or data or both have not yet been created!')\n",
    "        sys.exit(0)\n",
    "\n",
    "    # fit estimator on training data\n",
    "    print(\"Fitting the estimator on the training data...\")\n",
    "    estimator.fit(train_x, train_y)\n",
    "\n",
    "    print(\"Fitting complete.\")\n",
    "    print(train_x.shape)\n",
    "    print(test_x.shape)\n",
    "\n",
    "    # get predictions\n",
    "    print(\"Predicting for the test data...\")\n",
    "    preds = estimator.predict(test_x)\n",
    "\n",
    "    # make submission file\n",
    "    ids = test['id'].values\n",
    "    sub = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'attack': preds\n",
    "    })\n",
    "    sub.to_csv('../data/submission.csv', index=False)\n",
    "    print(\"Prediction complete. Submission file generated in the data folder.\")\n",
    "    print(f\"The submission has {sub.shape[0]} predictions\")"
   ]
  },
  {
   "source": [
    "## Best Deep Learning Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filename: deeplearning_transformer.py\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "This file contains code to create the transformers\n",
    "model that scored 0.782 on the public leaderboard\n",
    "\n",
    "The model uses a RoBERTa base pretrained architecture with\n",
    "a  RoBERTa model\n",
    "\n",
    "This code was run in Google Colab to make use of GPUs\n",
    "\n",
    "Therefore, running this code on a local system might throw issues\n",
    "with paths of data\n",
    "\n",
    "NOTE: Code has been written on the basis of documentation from\n",
    "https://simpletransformers.ai/\n",
    "\n",
    "The hyperparameter optimization process to decide the right values for\n",
    "learning rate and number of epochs is depicted in the transformer_hyp_opt.py\n",
    "file\n",
    "\n",
    "The colab file used is at\n",
    "https://colab.research.google.com/drive/1isI2ZvCre-J-1PXk-EGI4vdYyNd1fcrr?usp=sharing\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import sklearn\n",
    "\n",
    "# get data\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "sub = pd.read_csv(\"../data/sampleSubmission.csv\")\n",
    "\n",
    "# stratified cross validation\n",
    "df['kfold'] = -1\n",
    "\n",
    "target = df.attack.values\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "for fold, (trn_, val_) in enumerate(kf.split(X=df, y=target)):\n",
    "    df.loc[val_, 'kfold'] = fold \n",
    "\n",
    "df = df.drop(['id'], axis=1)\n",
    "df.columns = ['labels', 'text', 'kfold']\n",
    "\n",
    "# use fold 0 as the only fold for training\n",
    "# so essentially, this works like a holdout cross validation technique\n",
    "# using a 5-fold CV took a great computational toll on the machine\n",
    "train = df[df.kfold != 0]\n",
    "val = df[df.kfold == 0]\n",
    "\n",
    "# model configuration\n",
    "# best hyperparams set after hyperparam optimization\n",
    "# check ../models/hyp_opt_transformer.csv for the other possible options\n",
    "model_args = ClassificationArgs()\n",
    "model_args = {\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 0.000017,\n",
    "}\n",
    "\n",
    "# model\n",
    "model = ClassificationModel(\n",
    "    \"roberta\", \"roberta-base\", args=model_args\n",
    ")\n",
    "\n",
    "# train\n",
    "model.train_model(train, f1=sklearn.metrics.f1_score)\n",
    "\n",
    "# validate\n",
    "result, model_outputs, wrong_predictions = model.eval_model(val, f1=sklearn.metrics.f1_score)\n",
    "\n",
    "# making a submission\n",
    "test_comments = list(test['text'].values)\n",
    "predictions, raw_outputs = model.predict(test_comments)\n",
    "sub['attack'] = predictions\n",
    "sub.to_csv(\"transformer_submission.csv\", index=False)"
   ]
  }
 ]
}